{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction RAG - Full Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline with:\n",
    "- Batch processing of multiple drawings\n",
    "- LLM-powered summaries\n",
    "- Question answering\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Set your OpenRouter API key for LLM features:\n",
    "```bash\n",
    "export OPENROUTER_API_KEY=\"your-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from construction_rag import ConstructionRAGPipeline\n",
    "\n",
    "# Check if API key is set\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "print(f\"OpenRouter API key: {'Set ✓' if api_key else 'Not set ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Pipeline with LLM Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ConstructionRAGPipeline(\n",
    "    persist_directory=\"./full_pipeline_db\",\n",
    "    enable_summaries=True,  # Enable LLM summaries\n",
    "    llm_model=\"openai/gpt-4o-mini\",  # Fast and cost-effective\n",
    "    cluster_eps=0.02,  # DBSCAN epsilon\n",
    "    cluster_min_samples=2\n",
    ")\n",
    "\n",
    "print(f\"LLM enabled: {pipeline.llm is not None}\")\n",
    "if pipeline.llm:\n",
    "    print(f\"LLM model: {pipeline.llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Process Multiple Drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sample images\n",
    "import glob\n",
    "\n",
    "image_paths = glob.glob(\"sample_images/*.jpg\")\n",
    "print(f\"Found {len(image_paths)} sample images:\")\n",
    "for path in image_paths:\n",
    "    print(f\"  - {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images\n",
    "results = pipeline.process_batch(image_paths, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of processing\n",
    "successful = sum(1 for r in results if r.success)\n",
    "total_chunks = sum(len(r.chunks) for r in results)\n",
    "total_time = sum(r.processing_time for r in results)\n",
    "\n",
    "print(f\"\\nProcessing Summary:\")\n",
    "print(f\"  Images processed: {successful}/{len(results)}\")\n",
    "print(f\"  Total chunks: {total_chunks}\")\n",
    "print(f\"  Total time: {total_time:.1f}s\")\n",
    "print(f\"  Average time/image: {total_time/len(results):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine LLM-Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show chunks with their summaries\n",
    "all_chunks = [c for r in results for c in r.chunks]\n",
    "\n",
    "print(\"Sample chunks with summaries:\\n\")\n",
    "for chunk in all_chunks[:5]:\n",
    "    print(f\"[{chunk.chunk_type}] {chunk.chunk_id}\")\n",
    "    print(f\"  Content: {chunk.content[:60].replace(chr(10), ' ')}...\")\n",
    "    if chunk.summary:\n",
    "        print(f\"  Summary: {chunk.summary}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for specific content\n",
    "queries = [\n",
    "    \"door schedule fire rating\",\n",
    "    \"general notes\",\n",
    "    \"project information\",\n",
    "    \"floor plan layout\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    results = pipeline.query(query, n_results=2)\n",
    "    for r in results:\n",
    "        print(f\"\\n[{r.metadata['chunk_type']}] Score: {r.relevance_score:.3f}\")\n",
    "        print(f\"Source: {r.metadata.get('source_image', 'Unknown')}\")\n",
    "        summary = r.metadata.get('summary', '')\n",
    "        if summary:\n",
    "            print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Question Answering with LLM\n",
    "\n",
    "Use the `ask()` method to get LLM-generated answers grounded in the retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the drawings\n",
    "questions = [\n",
    "    \"What types of doors are mentioned in the drawings?\",\n",
    "    \"What are the general construction notes?\",\n",
    "    \"What is the project name?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        answer = pipeline.ask(question)\n",
    "        print(f\"\\nA: {answer}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"(LLM not available - set OPENROUTER_API_KEY)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pipeline.get_stats()\n",
    "\n",
    "print(\"Final Statistics:\")\n",
    "print(f\"  Total chunks indexed: {stats['total_chunks']}\")\n",
    "print(f\"  Embedding model: {stats['embedding_model']}\")\n",
    "print(f\"  Embedding dimension: {stats['embedding_dimension']}\")\n",
    "print(f\"  LLM model: {stats.get('llm_model', 'None')}\")\n",
    "print(f\"\\n  Chunks by type:\")\n",
    "for chunk_type, count in stats['chunks_by_type'].items():\n",
    "    print(f\"    {chunk_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear the database\n",
    "# pipeline.clear()\n",
    "# print(\"Database cleared!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
