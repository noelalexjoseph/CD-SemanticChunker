{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction RAG - Basic Usage\n",
    "\n",
    "This notebook demonstrates the basic usage of the Construction RAG library for processing construction drawings and performing semantic search.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install construction-rag\n",
    "```\n",
    "\n",
    "For LLM features, set your OpenRouter API key:\n",
    "```bash\n",
    "export OPENROUTER_API_KEY=\"your-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main pipeline\n",
    "from construction_rag import ConstructionRAGPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Pipeline\n",
    "\n",
    "Create a pipeline instance. If you don't have an OpenRouter API key, set `enable_summaries=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with summaries disabled (no API key required)\n",
    "pipeline = ConstructionRAGPipeline(\n",
    "    persist_directory=\"./demo_db\",\n",
    "    enable_summaries=False  # Set to True if you have OPENROUTER_API_KEY\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized!\")\n",
    "print(f\"Database location: {pipeline.persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process a Construction Drawing\n",
    "\n",
    "Process a sample construction drawing image. This will:\n",
    "1. Run IBM Docling for layout detection\n",
    "2. Apply DBSCAN clustering to group text blocks\n",
    "3. Index the chunks in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample image\n",
    "result = pipeline.process(\"sample_images/sample_floor_plan_1.jpg\")\n",
    "\n",
    "print(f\"\\nProcessing Result:\")\n",
    "print(f\"  Source: {result.source_image}\")\n",
    "print(f\"  Success: {result.success}\")\n",
    "print(f\"  Chunks extracted: {len(result.chunks)}\")\n",
    "print(f\"  Processing time: {result.processing_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the Extracted Chunks\n",
    "\n",
    "Let's look at the different types of chunks that were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count chunks by type\n",
    "from collections import Counter\n",
    "\n",
    "type_counts = Counter(c.chunk_type for c in result.chunks)\n",
    "print(\"Chunks by type:\")\n",
    "for chunk_type, count in type_counts.items():\n",
    "    print(f\"  {chunk_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "for chunk in result.chunks[:5]:\n",
    "    content_preview = chunk.content[:80].replace('\\n', ' ')\n",
    "    if len(chunk.content) > 80:\n",
    "        content_preview += \"...\"\n",
    "    print(f\"\\n[{chunk.chunk_type}] {chunk.chunk_id}\")\n",
    "    print(f\"  Content: {content_preview}\")\n",
    "    print(f\"  Confidence: {chunk.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query the Indexed Content\n",
    "\n",
    "Now we can perform semantic search to find relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a semantic search\n",
    "query = \"door schedule\"\n",
    "results = pipeline.query(query, n_results=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    content_preview = r.content[:100].replace('\\n', ' ')\n",
    "    print(f\"{i}. [{r.metadata['chunk_type']}] Score: {r.relevance_score:.3f}\")\n",
    "    print(f\"   {content_preview}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query\n",
    "query = \"project information\"\n",
    "results = pipeline.query(query, n_results=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r.metadata['chunk_type']}] Score: {r.relevance_score:.3f}\")\n",
    "    print(f\"   {r.content[:100].replace(chr(10), ' ')}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter by Chunk Type\n",
    "\n",
    "You can also filter queries to specific chunk types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query only tables\n",
    "results = pipeline.query(\"schedule\", n_results=3, filter_type=\"table\")\n",
    "\n",
    "print(\"Tables matching 'schedule':\")\n",
    "for r in results:\n",
    "    print(f\"  - {r.content[:60].replace(chr(10), ' ')}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Statistics\n",
    "\n",
    "Get statistics about the indexed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pipeline.get_stats()\n",
    "\n",
    "print(\"Pipeline Statistics:\")\n",
    "print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"  Embedding model: {stats['embedding_model']}\")\n",
    "print(f\"  LLM enabled: {stats['llm_enabled']}\")\n",
    "print(f\"\\n  Chunks by type:\")\n",
    "for chunk_type, count in stats['chunks_by_type'].items():\n",
    "    print(f\"    {chunk_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean Up\n",
    "\n",
    "Clear the database when done (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear the database\n",
    "# pipeline.clear()\n",
    "# print(\"Database cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See `02_full_pipeline.ipynb` for processing multiple drawings and using LLM features\n",
    "- See `03_evaluation.ipynb` for evaluating RAG quality with RAGAS metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
