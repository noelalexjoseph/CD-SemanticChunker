{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction RAG - Evaluation with RAGAS\n",
    "\n",
    "This notebook demonstrates how to evaluate the RAG pipeline using RAGAS-inspired metrics:\n",
    "- Context Precision\n",
    "- Context Recall\n",
    "- F1 Score\n",
    "- Keyword Coverage\n",
    "\n",
    "With LLM enabled, additional metrics:\n",
    "- Faithfulness\n",
    "- Answer Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from construction_rag import ConstructionRAGPipeline, ConstructionDrawingRAG\n",
    "from construction_rag.evaluation import RAGEvaluator, TEST_CASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Index Some Documents First\n",
    "\n",
    "Make sure you have some documents indexed before running evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline and process sample images\n",
    "pipeline = ConstructionRAGPipeline(\n",
    "    persist_directory=\"./eval_db\",\n",
    "    enable_summaries=False  # Faster for demo\n",
    ")\n",
    "\n",
    "# Process sample images if not already done\n",
    "import glob\n",
    "images = glob.glob(\"sample_images/*.jpg\")\n",
    "\n",
    "if pipeline.get_stats()['total_chunks'] == 0:\n",
    "    print(\"Processing sample images...\")\n",
    "    results = pipeline.process_batch(images, verbose=True)\n",
    "else:\n",
    "    print(f\"Using existing index with {pipeline.get_stats()['total_chunks']} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. View Test Cases\n",
    "\n",
    "The evaluation uses predefined test cases with ground truth information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of test cases: {len(TEST_CASES)}\\n\")\n",
    "\n",
    "for i, tc in enumerate(TEST_CASES[:3], 1):\n",
    "    print(f\"{i}. {tc['question']}\")\n",
    "    print(f\"   Expected types: {tc['relevant_chunk_types']}\")\n",
    "    print(f\"   Keywords: {tc['keywords']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Evaluator\n",
    "\n",
    "Create the evaluator with the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RAG component\n",
    "rag = pipeline.rag\n",
    "\n",
    "# Initialize evaluator (without LLM for basic metrics)\n",
    "evaluator = RAGEvaluator(rag, llm=None)\n",
    "\n",
    "print(\"Evaluator initialized\")\n",
    "print(f\"  RAG collection: {rag.collection_name}\")\n",
    "print(f\"  Chunks indexed: {rag.get_stats()['total_chunks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all test cases\n",
    "results = evaluator.evaluate_all(TEST_CASES, n_results=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics\n",
    "metrics = results['aggregate_metrics']\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTest cases: {metrics['num_test_cases']}\")\n",
    "print(f\"\\nRetrieval Metrics:\")\n",
    "print(f\"  Context Precision: {metrics['mean_context_precision']:.2%}\")\n",
    "print(f\"  Context Recall:    {metrics['mean_context_recall']:.2%}\")\n",
    "print(f\"  F1 Score:          {metrics['f1_score']:.2%}\")\n",
    "print(f\"  Keyword Coverage:  {metrics['mean_keyword_coverage']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INDIVIDUAL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, result in enumerate(results['individual_results'], 1):\n",
    "    print(f\"\\n{i}. {result['question'][:50]}...\")\n",
    "    print(f\"   Precision: {result['context_precision']:.2f}\")\n",
    "    print(f\"   Recall:    {result['context_recall']:.2f}\")\n",
    "    print(f\"   Keywords:  {result['keyword_coverage']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation\n",
    "\n",
    "Based on thesis evaluation thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuality Assessment:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Precision\n",
    "if metrics['mean_context_precision'] >= 0.7:\n",
    "    print(\"✓ Good precision: Retrieved chunks are mostly relevant\")\n",
    "elif metrics['mean_context_precision'] >= 0.4:\n",
    "    print(\"~ Moderate precision: Some irrelevant chunks retrieved\")\n",
    "else:\n",
    "    print(\"✗ Low precision: Many irrelevant chunks retrieved\")\n",
    "\n",
    "# Recall\n",
    "if metrics['mean_context_recall'] >= 0.5:\n",
    "    print(\"✓ Good recall: Most relevant chunk types found\")\n",
    "elif metrics['mean_context_recall'] >= 0.3:\n",
    "    print(\"~ Moderate recall: Some relevant types missing\")\n",
    "else:\n",
    "    print(\"✗ Low recall: Many relevant chunk types not found\")\n",
    "\n",
    "# F1\n",
    "if metrics['f1_score'] >= 0.6:\n",
    "    print(f\"✓ Good F1 score: {metrics['f1_score']:.2%} (threshold: 60%)\")\n",
    "else:\n",
    "    print(f\"~ Below threshold: {metrics['f1_score']:.2%} (threshold: 60%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. With LLM Metrics (Optional)\n",
    "\n",
    "If you have an OpenRouter API key, you can enable full RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run with LLM metrics\n",
    "\n",
    "# from construction_rag import OpenRouterLLM\n",
    "# \n",
    "# if os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "#     llm = OpenRouterLLM()\n",
    "#     evaluator_llm = RAGEvaluator(rag, llm=llm)\n",
    "#     results_llm = evaluator_llm.evaluate_all(TEST_CASES[:3])  # Just 3 for demo\n",
    "#     \n",
    "#     print(\"\\nLLM Metrics:\")\n",
    "#     print(f\"  Faithfulness:      {results_llm['aggregate_metrics']['mean_faithfulness']:.2%}\")\n",
    "#     print(f\"  Answer Relevancy:  {results_llm['aggregate_metrics']['mean_answer_relevancy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save results to file\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
